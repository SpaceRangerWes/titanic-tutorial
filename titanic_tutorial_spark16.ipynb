{"cells":[{"cell_type":"code","source":["import plotly \nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport numpy as np\n\nplotly.tools.set_credentials_file(username='', api_key='')"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["## Creating a DataFrame\nOften we are going to be importing data in a CSV format that conforms to some schema. We would like the ability to keep the dimensions and feature headers of that schema. Spark 1.6 has finally included the magic that is Pandas' Dataframes. These are essentially a database table but have built in operations (easily usable with numpy), dataflow optimizations, and easy data exploration.\n\n#### SQLContext\n[SQLContext](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.SQLContext) is the main entry point for Spark SQL functionality. A SQLContext can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. The SQLContext takes a [SparkContext](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.SparkContext) as a parameter and is in a manner of speaking a child of SparkContext.\n```python\ntrain_data_df = sqlContext.read.format('com.databricks.spark.csv').options(header='true',inferSchema='true').load('/FileStore/tables/libwrcg41471805568157/train.csv')\n```\nThis code generates a dataframe by generating a SQLContext. We pass the context some options that inform the Context a header row exists in our CSV and to automatically infer our datatypes. We then load these files from a file I have uploaded to my Databricks Community Edition workspace.\n\nLet's [check the schema](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html?highlight=printschema#pyspark.sql.DataFrame.printSchema) it has inferred and see if we agree with Spark's assumptions."],"metadata":{}},{"cell_type":"code","source":["train_data_df = (sqlContext\n                 .read\n                 .format('com.databricks.spark.csv')\n                 .options(header='true', inferSchema='true')\n                 .load('/FileStore/tables/libwrcg41471805568157/train.csv'))\ntrain_data_df.cache()\ntrain_data_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## What Happens Now?\nThere are several steps in the workflow for creating information out of data. Remember that data and information are not synonymous. Information is data in a comprehensible context. Big Data is by definition incomprehensible at a human level. It is not information that we can consume until that data is processed, cleaned, analyized, and visualized. Our understanding comes from regressions, models, statistics, and charting.\n\nCurrently we are dealing with raw data. Let's do some exploration on our new Dataframe to see what we are dealing with.\n\n#### Data Exploration\nThis is the first step we want to take before we start doing any pre-processing.\n\nWhy is that?\n\nData exploration will allow us to ask the questions of what steps we want to take to clean our data and to normalize our data. Maybe some features are missing in a few data objects. Maybe a date in one object is in a different format than in another object (i.e. 2016/08/22 vs August 22nd, 2016). These are serious issues that we need to address so that when we start doing some analysis, we can compare values to one-another. This process in particular is called Entity Resolution.\n\nIn Databricks notebook, all we need to do is call `display(dataframe)`. Super easy, right?\nThis will show us by default a table printout of a prefix of our full dataframe. You can also do other data exploration with the graphing feature.\n\n<img src=\"http://i.imgur.com/zxuRWlbl.png?2\" alt=\"Drawing\" />"],"metadata":{}},{"cell_type":"code","source":["train_data_df.describe().show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["display(train_data_df)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["#### Data Cleaning\nWhat is something that you noticed that could cause issues?\nHopefully, you noticed that some data objects (objects here are rows which represent a passenger) have a null age. This is going to cause some issues. How can we fix these non-values without affecting any population properties of the data? Let's use some [**domain knowledge**](http://www.simafore.com/blog/the-value-of-domain-knowledge-in-data-science) to do some more data exploration. Let's see if age follows any trends based off of class and/or ticket fare."],"metadata":{}},{"cell_type":"code","source":["explore_age_df = train_data_df.orderBy('Age', ascending=True)\n\navg_age_df = explore_age_df.where(explore_age_df['Age'].isNotNull()).groupBy('Pclass').avg('Age')\navg_age_df = avg_age_df.select('Pclass', avg_age_df['avg(Age)'].alias('Age'))\ndisplay(avg_age_df)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["###### Average Age vs. Class\nWe've barely done any exploration, and we can already possibly see a correlation between the average age of the passenger and their passenger class. We could easily use these averages to fill in missing data or we can keep looking for greater identifiers to try to eliminate bias."],"metadata":{}},{"cell_type":"code","source":["null_age_df = explore_age_df.where(explore_age_df['Age'].isNull())\nnull_age_df.describe().show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["p_class_df = (null_age_df\n              .select('Pclass')\n              .distinct()\n              .collect())\np_class_count_df = (null_age_df\n                    .groupBy('Pclass')\n                    .count()\n                    .select('count')\n                    .collect())\np_class_non_null_df = (explore_age_df\n                       .where(explore_age_df['Age'].isNotNull())\n                       .select('Pclass')\n                       .distinct()\n                       .collect())\np_class_count_non_null_df = (explore_age_df\n                             .where(explore_age_df['Age'].isNotNull())\n                             .groupBy('Pclass')\n                             .count()\n                             .select('count')\n                             .collect())\n\ndata = [\n    go.Bar(\n        x= p_class_df,\n        y= p_class_count_df\n    ),\n    go.Bar(\n        x= p_class_non_null_df,\n        y= p_class_count_non_null_df\n    )\n]\n\nurl = py.plot(data, filename='p-class-count', yTitle='# of passengers', xTitle='Passenger Class', title='Share of Passengers per class')"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["###### Visualization of Above\nFrom this point, we can see that the passenger class of the records where `Age = null` is approximately that of the records where `Age` has a value.\n<img src=\"http://i.imgur.com/6I3My9Cl.png\" alt=\"Drawing\" />\n\nAfter this exploration into the age distribution, we will just fill in the missing Age values with the average age per class."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import when, col\n\navg_age_list = avg_age_df.collect()\n\n# Replace null values with the average age values from our passenger class list\ndata_with_age_df = (train_data_df\n                     .select('*', \n                             when(train_data_df['Age'].isNull() & (train_data_df['Pclass'] == 1), \n                                  avg_age_list[0].Age)\n                             .otherwise(when(train_data_df['Age'].isNull() & (train_data_df['Pclass'] == 1), \n                                             avg_age_list[1].Age)\n                                        .otherwise(when(train_data_df['Age'].isNull() & (train_data_df['Pclass'] == 3), \n                                                        avg_age_list[2].Age)\n                                                   .otherwise(col('Age')))).alias('FilledAge')))\n\n# Replace the Age column values with those from our FilledAge column and then drop FilledAge.\ndata_with_age_df = data_with_age_df.withColumn('Age', data_with_age_df['FilledAge']).drop('FilledAge')"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["###### We Want Numbers and Not String Classifiers\nNumbers can be used much better than string features because they can be easily classified, regressed upon, and used by most ML models. Let's change the Sex feature into a numerical representation where `Sex = {0,1}`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\ndef sex_to_int(sex):\n  if(sex.lower() == 'male'):\n    return 0\n  else:\n    return 1\nsex_classify = udf(sex_to_int, IntegerType())\n\n# Using our user defined function (udf) we can map the string values of male and female to 0 and 1. This can also be done with the replace function.\nsex_int_df = data_with_age_df.select('*', sex_classify(data_with_age_df['Sex']).alias('IntSex'))\nclean_data_df = sex_int_df.withColumn('Sex', sex_int_df['IntSex']).drop('IntSex').cache()\ndisplay(clean_data_df)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["## Feature Engineering"],"metadata":{}}],"metadata":{"name":"Titanic Tutorial","notebookId":338688711146068},"nbformat":4,"nbformat_minor":0}
