{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly \n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "\n",
    "plotly.tools.set_credentials_file(username='', api_key='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame\n",
    "Often we are going to be importing data in a CSV format that conforms to some schema. We would like the ability to keep the dimensions and feature headers of that schema. Spark 1.6 has finally included the magic that is Pandas' Dataframes. These are essentially a database table but have built in operations (easily usable with numpy), dataflow optimizations, and easy data exploration.\n",
    "\n",
    "#### SQLContext\n",
    "[SQLContext](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.SQLContext) is the main entry point for Spark SQL functionality. A SQLContext can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. The SQLContext takes a [SparkContext](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.SparkContext) as a parameter and is in a manner of speaking a child of SparkContext.\n",
    "```python\n",
    "train_data_df = sqlContext.read.format('com.databricks.spark.csv').options(header='true',inferSchema='true').load('/FileStore/tables/libwrcg41471805568157/train.csv')\n",
    "```\n",
    "This code generates a dataframe by generating a SQLContext. We pass the context some options that inform the Context a header row exists in our CSV and to automatically infer our datatypes. We then load these files from a file I have uploaded to my Databricks Community Edition workspace.\n",
    "\n",
    "Let's [check the schema](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html?highlight=printschema#pyspark.sql.DataFrame.printSchema) it has inferred and see if we agree with Spark's assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_df = (sqlContext\n",
    "                 .read\n",
    "                 .format('com.databricks.spark.csv')\n",
    "                 .options(header='true', inferSchema='true')\n",
    "                 .load('/FileStore/tables/libwrcg41471805568157/train.csv'))\n",
    "train_data_df.cache()\n",
    "train_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Happens Now?\n",
    "There are several steps in the workflow for creating information out of data. Remember that data and information are not synonymous. Information is data in a comprehensible context. Big Data is by definition incomprehensible at a human level. It is not information that we can consume until that data is processed, cleaned, analyized, and visualized. Our understanding comes from regressions, models, statistics, and charting.\n",
    "\n",
    "Currently we are dealing with raw data. Let's do some exploration on our new Dataframe to see what we are dealing with.\n",
    "\n",
    "#### Data Exploration\n",
    "This is the first step we want to take before we start doing any pre-processing.\n",
    "\n",
    "Why is that?\n",
    "\n",
    "Data exploration will allow us to ask the questions of what steps we want to take to clean our data and to normalize our data. Maybe some features are missing in a few data objects. Maybe a date in one object is in a different format than in another object (i.e. 2016/08/22 vs August 22nd, 2016). These are serious issues that we need to address so that when we start doing some analysis, we can compare values to one-another. This process in particular is called Entity Resolution.\n",
    "\n",
    "In Databricks notebook, all we need to do is call `display(dataframe)`. Super easy, right?\n",
    "This will show us by default a table printout of a prefix of our full dataframe. You can also do other data exploration with the graphing feature.\n",
    "\n",
    "<img src=\"http://i.imgur.com/zxuRWlbl.png?2\" alt=\"Drawing\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(train_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "What is something that you noticed that could cause issues?\n",
    "Hopefully, you noticed that some data objects (objects here are rows which represent a passenger) have a null age. This is going to cause some issues. How can we fix these non-values without affecting any population properties of the data? Let's use some [**domain knowledge**](http://www.simafore.com/blog/the-value-of-domain-knowledge-in-data-science) to do some more data exploration. Let's see if age follows any trends based off of class and/or ticket fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explore_age_df = train_data_df.orderBy('Age', ascending=True).cache()\n",
    "\n",
    "avg_age_df = explore_age_df.where(explore_age_df['Age'].isNotNull()).groupBy('Pclass').avg('Age')\n",
    "display(avg_age_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Average Age vs. Class\n",
    "We've barely done any exploration, and we can already possibly see a correlation between the average age of the passenger and their passenger class. We could easily use these averages to fill in missing data or we can keep looking for greater identifiers to try to eliminate bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_age_df = explore_age_df.where(explore_age_df['Age'].isNull()).cache()\n",
    "null_age_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_class_df = (null_age_df\n",
    "              .select('Pclass')\n",
    "              .distinct()\n",
    "              .collect())\n",
    "p_class_count_df = (null_age_df\n",
    "                    .groupBy('Pclass')\n",
    "                    .count()\n",
    "                    .select('count')\n",
    "                    .collect())\n",
    "p_class_non_null_df = (explore_age_df\n",
    "                       .where(explore_age_df['Age'].isNotNull())\n",
    "                       .select('Pclass')\n",
    "                       .distinct()\n",
    "                       .collect())\n",
    "p_class_count_non_null_df = (explore_age_df\n",
    "                             .where(explore_age_df['Age'].isNotNull())\n",
    "                             .groupBy('Pclass')\n",
    "                             .count()\n",
    "                             .select('count')\n",
    "                             .collect())\n",
    "\n",
    "data = [\n",
    "    go.Bar(\n",
    "        x= p_class_df,\n",
    "        y= p_class_count_df\n",
    "    ),\n",
    "    go.Bar(\n",
    "        x= p_class_non_null_df,\n",
    "        y= p_class_count_non_null_df\n",
    "    )\n",
    "]\n",
    "\n",
    "url = py.plot(data, filename='p-class-count', yTitle='# of passengers', xTitle='Passenger Class', title='Share of Passengers per class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Visualization of Above\n",
    "From this point, we can see that the passenger class of the records where `Age = null` is approximately that of the records where `Age` has a value.\n",
    "<img src=\"http://i.imgur.com/6I3My9Cl.png\" alt=\"Drawing\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Analyzing a Dataframe of all 'null' Age datapoints.\n",
    "You can see above that I was interested in the statistics of data points that did not possess any value for age. It turns out that... unfortunately, there is hardly any difference. At this point, I'm just going to say I'm bored and want to start doing some modeling. Not the best idea but let's try it and see what level of accuracy we get."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "Titanic Tutorial",
  "notebookId": 338688711146068
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
